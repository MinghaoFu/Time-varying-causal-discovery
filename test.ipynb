{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "from scipy.stats import hypsecant\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def generate_mixing_matrix(d_sources: int, d_data=None, lin_type='uniform', cond_threshold=25, sparse_threshold=0.25, n_iter_4_cond=None,\n",
    "                           dtype=np.float32, staircase=False):\n",
    "    \"\"\"\n",
    "    Generate square linear mixing matrix\n",
    "    @param d_sources: dimension of the latent sources\n",
    "    @param d_data: dimension of the mixed data\n",
    "    @param lin_type: specifies the type of matrix entries; either `uniform` or `orthogonal`.\n",
    "    @param cond_threshold: higher bound on the condition number of the matrix to ensure well-conditioned problem\n",
    "    @param n_iter_4_cond: or instead, number of iteration to compute condition threshold of the mixing matrix.\n",
    "        cond_threshold is ignored in this case/\n",
    "    @param dtype: data type for data\n",
    "    @param staircase: if True, generate mixing that preserves staircase form of sources\n",
    "    @return:\n",
    "        A: mixing matrix\n",
    "    @rtype: np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    def _gen_matrix(ds, dd, dtype):\n",
    "        A = (np.random.uniform(0, 2, (ds, dd)) - 1).astype(dtype)\n",
    "        for i in range(dd):\n",
    "            A[:, i] /= np.sqrt((A[:, i] ** 2).sum())\n",
    "        return A\n",
    "\n",
    "    def _gen_matrix_staircase(ds, dd, dtype, sq=None):\n",
    "        if sq is None:\n",
    "            sq = dd > 2\n",
    "        A1 = np.zeros((ds, 1))  # first row of A should be e_1\n",
    "        A1[0, 0] = 1\n",
    "        A2 = np.random.uniform(0, 2, (ds, dd - 1)) - 1\n",
    "        if sq:\n",
    "            A2[0] = 0\n",
    "        A = np.concatenate([A1, A2], axis=1).astype(dtype)\n",
    "        for i in range(dd):\n",
    "            A[:, i] /= np.sqrt((A[:, i] ** 2).sum())\n",
    "        return A\n",
    "\n",
    "    if d_data is None:\n",
    "        d_data = d_sources\n",
    "\n",
    "    if lin_type == 'orthogonal':\n",
    "        A = (np.linalg.qr(np.random.uniform(-1, 1, (d_sources, d_data)))[0]).astype(dtype)\n",
    "\n",
    "    elif lin_type == 'uniform':\n",
    "        if n_iter_4_cond is None:\n",
    "            cond_thresh = cond_threshold\n",
    "        else:\n",
    "            cond_list = []\n",
    "            for _ in range(int(n_iter_4_cond)):\n",
    "                A = np.random.uniform(-1, 1, (d_sources, d_data)).astype(dtype)\n",
    "                for i in range(d_data):\n",
    "                    A[:, i] /= np.sqrt((A[:, i] ** 2).sum())\n",
    "                cond_list.append(np.linalg.cond(A))\n",
    "\n",
    "            cond_thresh = np.percentile(cond_list, 25)  # only accept those below 25% percentile\n",
    "\n",
    "        gen_mat = _gen_matrix if not staircase else _gen_matrix_staircase\n",
    "        A = gen_mat(d_sources, d_data, dtype)\n",
    "        while np.linalg.cond(A) > cond_thresh:\n",
    "            A = gen_mat(d_sources, d_data, dtype)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('incorrect method')\n",
    "    \n",
    "    A[np.abs(A) < sparse_threshold] = 0\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        -0.63767374, -0.5911209 ,  0.        ,  0.4373041 ,  0.        ],\n",
       "       [-0.6034208 ,  0.        ,  0.        , -0.4965698 , -0.6713974 ,\n",
       "        -0.4428673 ,  0.47094843, -0.6999715 ,  0.45942622, -0.6359788 ],\n",
       "       [-0.5133873 , -0.69003177, -0.72171533, -0.5761669 , -0.5656652 ,\n",
       "         0.50767124,  0.        , -0.45593968, -0.6239238 ,  0.5493428 ],\n",
       "       [-0.6069564 , -0.6747772 , -0.5946955 , -0.63055027, -0.47879654,\n",
       "         0.        , -0.53419226,  0.        ,  0.45652145, -0.5176245 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_mixing_matrix(4, 10, lin_type='uniform', cond_threshold=25, sparse_threshold=0.4, n_iter_4_cond=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for generating piece-wise stationary data.\n",
    "\n",
    "Each component of the independent latents is comprised of `ns` segments, and each segment has different parameters.\\\n",
    "Each segment has `nps` data points 9measurements).\n",
    "\n",
    "The latent components are then mixed by an MLP into observations (not necessarily of the same dimension.\n",
    "It is possible to add noise to the observations\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "from scipy.stats import hypsecant\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def to_one_hot(x, m=None):\n",
    "    if type(x) is not list:\n",
    "        x = [x]\n",
    "    if m is None:\n",
    "        ml = []\n",
    "        for xi in x:\n",
    "            ml += [xi.max() + 1]\n",
    "        m = max(ml)\n",
    "    dtp = x[0].dtype\n",
    "    xoh = []\n",
    "    for i, xi in enumerate(x):\n",
    "        xoh += [np.zeros((xi.size, int(m)), dtype=dtp)]\n",
    "        xoh[i][np.arange(xi.size), xi.astype(int)] = 1\n",
    "    return xoh\n",
    "\n",
    "\n",
    "def lrelu(x, neg_slope):\n",
    "    \"\"\"\n",
    "    Leaky ReLU activation function\n",
    "    @param x: input array\n",
    "    @param neg_slope: slope for negative values\n",
    "    @return:\n",
    "        out: output rectified array\n",
    "    \"\"\"\n",
    "\n",
    "    def _lrelu_1d(_x, _neg_slope):\n",
    "        \"\"\"\n",
    "        one dimensional implementation of leaky ReLU\n",
    "        \"\"\"\n",
    "        if _x > 0:\n",
    "            return _x\n",
    "        else:\n",
    "            return _x * _neg_slope\n",
    "\n",
    "    leaky1d = np.vectorize(_lrelu_1d)\n",
    "    assert neg_slope > 0  # must be positive\n",
    "    return leaky1d(x, neg_slope)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    @param x: input array\n",
    "    @return:\n",
    "        out: output array\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def generate_mixing_matrix(d_sources: int, d_data=None, lin_type='uniform', cond_threshold=25, n_iter_4_cond=None,\n",
    "                           dtype=np.float32, staircase=False):\n",
    "    \"\"\"\n",
    "    Generate square linear mixing matrix\n",
    "    @param d_sources: dimension of the latent sources\n",
    "    @param d_data: dimension of the mixed data\n",
    "    @param lin_type: specifies the type of matrix entries; either `uniform` or `orthogonal`.\n",
    "    @param cond_threshold: higher bound on the condition number of the matrix to ensure well-conditioned problem\n",
    "    @param n_iter_4_cond: or instead, number of iteration to compute condition threshold of the mixing matrix.\n",
    "        cond_threshold is ignored in this case/\n",
    "    @param dtype: data type for data\n",
    "    @param staircase: if True, generate mixing that preserves staircase form of sources\n",
    "    @return:\n",
    "        A: mixing matrix\n",
    "    @rtype: np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    def _gen_matrix(ds, dd, dtype):\n",
    "        A = (np.random.uniform(0, 2, (ds, dd)) - 1).astype(dtype)\n",
    "        for i in range(dd):\n",
    "            A[:, i] /= np.sqrt((A[:, i] ** 2).sum())\n",
    "        return A\n",
    "\n",
    "    def _gen_matrix_staircase(ds, dd, dtype, sq=None):\n",
    "        if sq is None:\n",
    "            sq = dd > 2\n",
    "        A1 = np.zeros((ds, 1))  # first row of A should be e_1\n",
    "        A1[0, 0] = 1\n",
    "        A2 = np.random.uniform(0, 2, (ds, dd - 1)) - 1\n",
    "        if sq:\n",
    "            A2[0] = 0\n",
    "        A = np.concatenate([A1, A2], axis=1).astype(dtype)\n",
    "        for i in range(dd):\n",
    "            A[:, i] /= np.sqrt((A[:, i] ** 2).sum())\n",
    "        return A\n",
    "\n",
    "    if d_data is None:\n",
    "        d_data = d_sources\n",
    "\n",
    "    if lin_type == 'orthogonal':\n",
    "        A = (np.linalg.qr(np.random.uniform(-1, 1, (d_sources, d_data)))[0]).astype(dtype)\n",
    "\n",
    "    elif lin_type == 'uniform':\n",
    "        if n_iter_4_cond is None:\n",
    "            cond_thresh = cond_threshold\n",
    "        else:\n",
    "            cond_list = []\n",
    "            for _ in range(int(n_iter_4_cond)):\n",
    "                A = np.random.uniform(-1, 1, (d_sources, d_data)).astype(dtype)\n",
    "                for i in range(d_data):\n",
    "                    A[:, i] /= np.sqrt((A[:, i] ** 2).sum())\n",
    "                cond_list.append(np.linalg.cond(A))\n",
    "\n",
    "            cond_thresh = np.percentile(cond_list, 25)  # only accept those below 25% percentile\n",
    "\n",
    "        gen_mat = _gen_matrix if not staircase else _gen_matrix_staircase\n",
    "        A = gen_mat(d_sources, d_data, dtype)\n",
    "        while np.linalg.cond(A) > cond_thresh:\n",
    "            A = gen_mat(d_sources, d_data, dtype)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('incorrect method')\n",
    "    return A\n",
    "\n",
    "\n",
    "def generate_nonstationary_sources(n_per_seg: int, n_seg: int, d: int, prior='gauss', var_bounds=np.array([0.5, 3]),\n",
    "                                   dtype=np.float32, uncentered=False, centers=None, staircase=False):\n",
    "    \"\"\"\n",
    "    Generate source signal following a TCL distribution. Within each segment, sources are independent.\n",
    "    The distribution withing each segment is given by the keyword `dist`\n",
    "    @param n_per_seg: number of points per segment\n",
    "    @param n_seg: number of segments\n",
    "    @param d: dimension of the sources same as data\n",
    "    @param prior: distribution of the sources. can be `lap` for Laplace , `hs` for Hypersecant or `gauss` for Gaussian\n",
    "    @param var_bounds: optional, upper and lower bounds for the modulation parameter\n",
    "    @param dtype: data type for data\n",
    "    @param bool uncentered: True to generate uncentered data\n",
    "    @param centers: if uncentered, pass the desired centers to this parameter. If None, the centers will be drawn\n",
    "                    at random\n",
    "    @param staircase: if True, s_1 will have a staircase form, used to break TCL.\n",
    "    @return:\n",
    "        sources: output source array of shape (n, d)\n",
    "        labels: label for each point; the label is the component\n",
    "        m: mean of each component\n",
    "        L: modulation parameter of each component\n",
    "    @rtype: (np.ndarray, np.ndarray, np.ndarray, np.ndarray)\n",
    "    \"\"\"\n",
    "    var_lb = var_bounds[0]\n",
    "    var_ub = var_bounds[1]\n",
    "    n = n_per_seg * n_seg\n",
    "\n",
    "    L = np.random.uniform(var_lb, var_ub, (n_seg, d))\n",
    "    if uncentered:\n",
    "        if centers is not None:\n",
    "            assert centers.shape == (n_seg, d)\n",
    "            m = centers\n",
    "        else:\n",
    "            m = np.random.uniform(-5, 5, (n_seg, d))\n",
    "    else:\n",
    "        m = np.zeros((n_seg, d))\n",
    "\n",
    "    if staircase:\n",
    "        m1 = 3 * np.arange(n_seg).reshape((-1, 1))\n",
    "        a = np.random.permutation(n_seg)\n",
    "        m1 = m1[a]\n",
    "        # L[:, 0] = .2\n",
    "        if uncentered:\n",
    "            m2 = np.random.uniform(-1, 1, (n_seg, d - 1))\n",
    "        else:\n",
    "            m2 = np.zeros((n_seg, d - 1))\n",
    "        m = np.concatenate([m1, m2], axis=1)\n",
    "\n",
    "    labels = np.zeros(n, dtype=dtype)\n",
    "    if prior == 'lap':\n",
    "        sources = np.random.laplace(0, 1 / np.sqrt(2), (n, d)).astype(dtype)\n",
    "    elif prior == 'hs':\n",
    "        sources = scipy.stats.hypsecant.rvs(0, 1, (n, d)).astype(dtype)\n",
    "    elif prior == 'gauss':\n",
    "        sources = np.random.randn(n, d).astype(dtype)\n",
    "    else:\n",
    "        raise ValueError('incorrect dist')\n",
    "\n",
    "    for seg in range(n_seg):\n",
    "        segID = range(n_per_seg * seg, n_per_seg * (seg + 1))\n",
    "        sources[segID] *= L[seg]\n",
    "        sources[segID] += m[seg]\n",
    "        labels[segID] = seg\n",
    "\n",
    "    return sources, labels, m, L\n",
    "\n",
    "\n",
    "def generate_data(n_per_seg, n_seg, d_sources, d_data=None, n_layers=3, prior='gauss', activation='lrelu', batch_size=0,\n",
    "                  seed=10, slope=.1, var_bounds=np.array([0.5, 3]), lin_type='uniform', n_iter_4_cond=1e4,\n",
    "                  dtype=np.float32, noisy=0, uncentered=False, centers=None, staircase=False, discrete=False,\n",
    "                  one_hot_labels=True, repeat_linearity=False):\n",
    "    \"\"\"\n",
    "    Generate artificial data with arbitrary mixing\n",
    "    @param int n_per_seg: number of observations per segment\n",
    "    @param int n_seg: number of segments\n",
    "    @param int d_sources: dimension of the latent sources\n",
    "    @param int or None d_data: dimension of the data\n",
    "    @param int n_layers: number of layers in the mixing MLP\n",
    "    @param str activation: activation function for the mixing MLP; can be `none, `lrelu`, `xtanh` or `sigmoid`\n",
    "    @param str prior: prior distribution of the sources; can be `lap` for Laplace or `hs` for Hypersecant\n",
    "    @param int batch_size: batch size if data is to be returned as batches. 0 for a single batch of size n\n",
    "    @param int seed: random seed\n",
    "    @param var_bounds: upper and lower bounds for the modulation parameter\n",
    "    @param float slope: slope parameter for `lrelu` or `xtanh`\n",
    "    @param str lin_type: specifies the type of matrix entries; can be `uniform` or `orthogonal`\n",
    "    @param int n_iter_4_cond: number of iteration to compute condition threshold of the mixing matrix\n",
    "    @param dtype: data type for data\n",
    "    @param float noisy: if non-zero, controls the level of noise added to observations\n",
    "    @param bool uncentered: True to generate uncentered data\n",
    "    @param np.ndarray centers: array of centers if uncentered == True\n",
    "    @param bool staircase: if True, generate staircase data\n",
    "    @param bool one_hot_labels: if True, transform labels into one-hot vectors\n",
    "\n",
    "    @return:\n",
    "        tuple of batches of generated (sources, data, auxiliary variables, mean, variance)\n",
    "    @rtype: tuple\n",
    "\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if d_data is None:\n",
    "        d_data = d_sources\n",
    "\n",
    "    # sources\n",
    "    S, U, M, L = generate_nonstationary_sources(n_per_seg, n_seg, d_sources, prior=prior,\n",
    "                                                var_bounds=var_bounds, dtype=dtype,\n",
    "                                                uncentered=uncentered, centers=centers, staircase=staircase)\n",
    "    n = n_per_seg * n_seg\n",
    "\n",
    "    # non linearity\n",
    "    if activation == 'lrelu':\n",
    "        act_f = lambda x: lrelu(x, slope).astype(dtype)\n",
    "    elif activation == 'sigmoid':\n",
    "        act_f = sigmoid\n",
    "    elif activation == 'xtanh':\n",
    "        act_f = lambda x: np.tanh(x) + slope * x\n",
    "    elif activation == 'none':\n",
    "        act_f = lambda x: x\n",
    "    else:\n",
    "        raise ValueError('incorrect non linearity: {}'.format(activation))\n",
    "\n",
    "    # Mixing time!\n",
    "\n",
    "    if not repeat_linearity:\n",
    "        X = S.copy()\n",
    "        for nl in range(n_layers):\n",
    "            A = generate_mixing_matrix(X.shape[1], d_data, lin_type=lin_type, n_iter_4_cond=n_iter_4_cond, dtype=dtype,\n",
    "                                       staircase=staircase)\n",
    "            if nl == n_layers - 1:\n",
    "                X = np.dot(X, A)\n",
    "            else:\n",
    "                X = act_f(np.dot(X, A))\n",
    "\n",
    "    else:\n",
    "        assert n_layers > 1  # suppose we always have at least 2 layers. The last layer doesn't have a non-linearity\n",
    "        A = generate_mixing_matrix(d_sources, d_data, lin_type=lin_type, n_iter_4_cond=n_iter_4_cond, dtype=dtype)\n",
    "        X = act_f(np.dot(S, A))\n",
    "        if d_sources != d_data:\n",
    "            B = generate_mixing_matrix(d_data, lin_type=lin_type, n_iter_4_cond=n_iter_4_cond, dtype=dtype)\n",
    "        else:\n",
    "            B = A\n",
    "        for nl in range(1, n_layers):\n",
    "            if nl == n_layers - 1:\n",
    "                X = np.dot(X, B)\n",
    "            else:\n",
    "                X = act_f(np.dot(X, B))\n",
    "\n",
    "    # add noise:\n",
    "    if noisy:\n",
    "        X += noisy * np.random.randn(*X.shape)\n",
    "\n",
    "    if discrete:\n",
    "        X = np.random.binomial(1, sigmoid(X))\n",
    "\n",
    "    if not batch_size:\n",
    "        if one_hot_labels:\n",
    "            U = to_one_hot([U], m=n_seg)[0]\n",
    "        return S, X, U, M, L\n",
    "    else:\n",
    "        idx = np.random.permutation(n)\n",
    "        Xb, Sb, Ub, Mb, Lb = [], [], [], [], []\n",
    "        n_batches = int(n / batch_size)\n",
    "        for c in range(n_batches):\n",
    "            Sb += [S[idx][c * batch_size:(c + 1) * batch_size]]\n",
    "            Xb += [X[idx][c * batch_size:(c + 1) * batch_size]]\n",
    "            Ub += [U[idx][c * batch_size:(c + 1) * batch_size]]\n",
    "            Mb += [M[idx][c * batch_size:(c + 1) * batch_size]]\n",
    "            Lb += [L[idx][c * batch_size:(c + 1) * batch_size]]\n",
    "        if one_hot_labels:\n",
    "            Ub = to_one_hot(Ub, m=n_seg)\n",
    "        return Sb, Xb, Ub, Mb, Lb\n",
    "\n",
    "\n",
    "def save_data(path, *args, **kwargs):\n",
    "    kwargs['batch_size'] = 0  # leave batch creation to torch DataLoader\n",
    "    S, X, U, M, L = generate_data(*args, **kwargs)\n",
    "    print('Creating dataset {} ...'.format(path))\n",
    "    dir_path = '/'.join(path.split('/')[:-1])\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs('/'.join(path.split('/')[:-1]))\n",
    "    np.savez_compressed(path, s=S, x=X, u=U, m=M, L=L)\n",
    "    print(' ... done')\n",
    "\n",
    "\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, root, nps, ns, dl, dd, nl, s, p, a, uncentered=False, noisy=False, centers=None, double=False,\n",
    "                 one_hot_labels=True):\n",
    "        self.root = root\n",
    "        data = self.load_tcl_data(root, nps, ns, dl, dd, nl, s, p, a, uncentered, noisy, centers, one_hot_labels)\n",
    "        self.data = data\n",
    "        self.s = torch.from_numpy(data['s'])\n",
    "        self.x = torch.from_numpy(data['x'])\n",
    "        self.u = torch.from_numpy(data['u'])\n",
    "        self.l = data['L']\n",
    "        self.m = data['m']\n",
    "        self.len = self.x.shape[0]\n",
    "        self.latent_dim = self.s.shape[1]\n",
    "        self.aux_dim = self.u.shape[1]\n",
    "        self.data_dim = self.x.shape[1]\n",
    "        self.prior = p\n",
    "        self.activation = a\n",
    "        self.seed = s\n",
    "        self.n_layers = nl\n",
    "        self.uncentered = uncentered\n",
    "        self.noisy = noisy\n",
    "        self.double = double\n",
    "        self.one_hot_labels = one_hot_labels\n",
    "\n",
    "    def get_dims(self):\n",
    "        return self.data_dim, self.latent_dim, self.aux_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if not self.double:\n",
    "            return self.x[index], self.u[index], self.s[index]\n",
    "        else:\n",
    "            indices = range(len(self))\n",
    "            index2 = np.random.choice(indices)\n",
    "            return self.x[index], self.x[index2], self.u[index], self.s[index]\n",
    "\n",
    "    @staticmethod\n",
    "    def load_tcl_data(root, nps, ns, dl, dd, nl, s, p, a, uncentered, noisy, centers, one_hot_labels):\n",
    "        path_to_dataset = root + 'tcl_' + '_'.join(\n",
    "            [str(nps), str(ns), str(dl), str(dd), str(nl), str(s), p, a])\n",
    "        if uncentered:\n",
    "            path_to_dataset += '_u'\n",
    "        if noisy:\n",
    "            path_to_dataset += '_noisy'\n",
    "        path_to_dataset += '.npz'\n",
    "\n",
    "        if not os.path.exists(path_to_dataset) or s is None:\n",
    "            kwargs = {\"n_per_seg\": nps, \"n_seg\": ns, \"d_sources\": dl, \"d_data\": dd, \"n_layers\": nl, \"prior\": p,\n",
    "                      \"activation\": a, \"seed\": s, \"batch_size\": 0, \"uncentered\": uncentered, \"noisy\": noisy,\n",
    "                      \"centers\": centers, \"repeat_linearity\": True, \"one_hot_labels\": one_hot_labels}\n",
    "            save_data(path_to_dataset, **kwargs)\n",
    "        print('loading data from {}'.format(path_to_dataset))\n",
    "        return np.load(path_to_dataset)\n",
    "\n",
    "    def get_test_sample(self, batch_size, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        idx = np.random.randint(max(0, self.len - batch_size))\n",
    "        return self.x[idx:idx + batch_size], self.u[idx:idx + batch_size], self.s[idx:idx + batch_size]\n",
    "\n",
    "\n",
    "class CustomSyntheticDataset(Dataset):\n",
    "    def __init__(self, X, U, S=None, device='cpu'):\n",
    "        self.device = device\n",
    "        self.x = torch.from_numpy(X).to(self.device)\n",
    "        self.u = torch.from_numpy(U).to(self.device)\n",
    "        if S is not None:\n",
    "            self.s = torch.from_numpy(S).to(self.device)\n",
    "        else:\n",
    "            self.s = self.x\n",
    "        self.len = self.x.shape[0]\n",
    "        self.latent_dim = self.s.shape[1]\n",
    "        self.aux_dim = self.u.shape[1]\n",
    "        self.data_dim = self.x.shape[1]\n",
    "        self.nps = int(self.len / self.aux_dim)\n",
    "        print('data loaded on {}'.format(self.x.device))\n",
    "\n",
    "    def get_dims(self):\n",
    "        return self.data_dim, self.latent_dim, self.aux_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.u[index], self.s[index]\n",
    "\n",
    "    def get_metadata(self):\n",
    "        return {'nps': self.nps,\n",
    "                'ns': self.aux_dim,\n",
    "                'n': self.len,\n",
    "                'latent_dim': self.latent_dim,\n",
    "                'data_dim': self.data_dim,\n",
    "                'aux_dim': self.aux_dim,\n",
    "                }\n",
    "\n",
    "\n",
    "def create_if_not_exist_dataset(root='data/', nps=1000, ns=40, dl=2, dd=4, nl=3, s=1, p='gauss', a='xtanh',\n",
    "                                uncentered=False, noisy=False, arg_str=None):\n",
    "    \"\"\"\n",
    "    Create a dataset if it doesn't exist.\n",
    "    This is useful as a setup step when running multiple jobs in parallel, to avoid having many scripts attempting\n",
    "    to create the dataset when non-existent.\n",
    "    This is called in `cmd_utils.create_dataset_before`\n",
    "    \"\"\"\n",
    "    if arg_str is not None:\n",
    "        # overwrites all other arg values\n",
    "        # arg_str should be of this form: nps_ns_dl_dd_nl_s_p_a_u_n\n",
    "        arg_list = arg_str.split('\\n')[0].split('_')\n",
    "        print(arg_list)\n",
    "        assert len(arg_list) == 10\n",
    "        nps, ns, dl, dd, nl = map(int, arg_list[0:5])\n",
    "        p, a = arg_list[6:8]\n",
    "        if arg_list[5] == 'n':\n",
    "            s = None\n",
    "        else:\n",
    "            s = int(arg_list[5])\n",
    "        if arg_list[-2] == 'f':\n",
    "            uncentered = False\n",
    "        else:\n",
    "            uncentered = True\n",
    "        if arg_list[-1] == 'f':\n",
    "            noisy = False\n",
    "        else:\n",
    "            noisy = True\n",
    "\n",
    "    path_to_dataset = root + 'tcl_' + '_'.join(\n",
    "        [str(nps), str(ns), str(dl), str(dd), str(nl), str(s), p, a])\n",
    "    if uncentered:\n",
    "        path_to_dataset += '_u'\n",
    "    if noisy:\n",
    "        path_to_dataset += '_n'\n",
    "    path_to_dataset += '.npz'\n",
    "\n",
    "    if not os.path.exists(path_to_dataset) or s is None:\n",
    "        kwargs = {\"n_per_seg\": nps, \"n_seg\": ns, \"d_sources\": dl, \"d_data\": dd, \"n_layers\": nl, \"prior\": p,\n",
    "                  \"activation\": a, \"seed\": s, \"batch_size\": 0, \"uncentered\": uncentered, \"noisy\": noisy}\n",
    "        save_data(path_to_dataset, **kwargs)\n",
    "    return path_to_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sb, Xb, Ub, Mb, Lb = generate_data(1, 5, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.8015467e-01, -8.7306046e-01, -3.2407942e-01,  1.3270302e+00,\n",
       "        -1.1856440e+00,  1.1660393e+00,  1.8370596e-01, -1.0105953e+00,\n",
       "        -4.9888709e-01, -1.4616632e+00],\n",
       "       [-2.5258458e+00, -3.8444293e+00, -7.3371525e+00,  4.0416880e+00,\n",
       "        -5.2139914e-01, -4.8556662e-01,  2.5361075e+00, -4.0636373e+00,\n",
       "        -5.5086666e-01, -2.1927814e+00],\n",
       "       [-2.0437068e-01, -4.7709227e-01, -4.1299043e+00,  1.5543485e+00,\n",
       "         5.7578683e-01,  7.3924154e-01,  1.3668666e+00, -2.1696773e+00,\n",
       "         1.1897178e+00, -1.7874178e-01],\n",
       "       [ 1.0328923e-01,  1.3020010e-01,  2.4816841e-03, -9.2822120e-02,\n",
       "        -6.0757369e-02, -2.1214187e-03,  3.5399992e-02, -1.1697991e-01,\n",
       "         2.1394096e-02, -7.1993403e-02],\n",
       "       [-1.7105941e-01, -8.4500825e-01, -1.8436443e+00,  2.6000893e+00,\n",
       "        -4.4395214e-01,  1.7785010e+00,  1.0026376e+00, -6.9585961e-01,\n",
       "         1.0027802e+00, -8.7905884e-01]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
